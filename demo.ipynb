{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "from load_glove_matrix import Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained Glove vectors from file ./data/glove.840B.300d.txt\n",
      "  --9.11%  loaded.\n",
      "  --18.21%  loaded.\n",
      "  --27.32%  loaded.\n",
      "  --36.43%  loaded.\n",
      "  --45.54%  loaded.\n",
      "  --54.64%  loaded.\n",
      "  --63.75%  loaded.\n",
      "  --72.86%  loaded.\n",
      "  --81.97%  loaded.\n",
      "  --91.07%  loaded.\n",
      "Finished loading Glove model. 2196017 vectors loaded\n"
     ]
    }
   ],
   "source": [
    "# load pre-trained Glove vectors, 300d\n",
    "glove_model = Glove()\n",
    "glove_model.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all unique words and part of speach\n",
    "def find_words_pos(words, tags): \n",
    "    uniqueWords = []\n",
    "    posWords = []\n",
    "    for i in words:\n",
    "        if not i in uniqueWords:\n",
    "            uniqueWords.append(i)\n",
    "    for tag in tags:\n",
    "        for word in uniqueWords:\n",
    "            if word[1] == tag:\n",
    "                posWords.append(word)\n",
    "            else:\n",
    "                continue\n",
    "    return posWords\n",
    "\n",
    "def find_words(words, trigger):\n",
    "    l = []\n",
    "    for word in words:\n",
    "        if trigger not in ('word', 'pos'):\n",
    "            print(\"Invalid trigger should be 'word' or 'pos'\")\n",
    "            break\n",
    "        elif trigger == 'word':\n",
    "            l.append(word[0])\n",
    "        elif trigger == 'pos':\n",
    "            l.append(word[1])\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start preparing words...\n",
      "  --0.0%  loaded.\n",
      "  --10.0%  loaded.\n",
      "  --20.0%  loaded.\n",
      "  --30.0%  loaded.\n",
      "  --40.0%  loaded.\n",
      "  --50.0%  loaded.\n",
      "  --60.0%  loaded.\n",
      "  --70.0%  loaded.\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# make the twitter .csv as a .txt file and read all words from the file\n",
    "pos = []\n",
    "print(\"Start preparing words...\")\n",
    "# download CoreNLP 3.9.1 from https://stanfordnlp.github.io/CoreNLP/history.html\n",
    "nlp = StanfordCoreNLP(r'C:\\Users\\sdzar\\OneDrive\\Documents\\GitHub\\CGT470-Final\\data\\stanford-corenlp-full-2018-02-27',memory='8g')\n",
    "# this is the wiki corpus, you can change it to any other curpos you want\n",
    "with open(\"./data/sentence.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    for i, line in enumerate(file):\n",
    "        pos += nlp.pos_tag(line)\n",
    "        if (i+1) % 1 == 0:\n",
    "            print(\"  --{}%  loaded.\".format(round(i/10*100, 2)))\n",
    "nlp.close() # Do not forget to close! The backend server will consume a lot memery.\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pos 1: ('I', 'PRP')\n",
      "\n",
      "Pos 2: ('love', 'VBP')\n",
      "\n",
      "Pos 3: ('dogs', 'NNS')\n",
      "\n",
      "Pos 4: ('.', '.')\n",
      "\n",
      "Pos 5: ('I', 'PRP')\n",
      "\n",
      "Pos 6: ('hate', 'VBP')\n",
      "\n",
      "Pos 7: ('dogs', 'NNS')\n",
      "\n",
      "Pos 8: ('.', '.')\n",
      "\n",
      "Pos 9: ('I', 'PRP')\n",
      "\n",
      "Pos 10: ('like', 'VBP')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# examples of result\n",
    "for i in range(0,10):\n",
    "    print(\"Pos {}: {}\\n\".format(i+1,pos[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique: \n",
      " [('police', 'NN'), ('puma', 'NN'), ('night', 'NN'), ('cat', 'NN'), ('yesterday', 'NN'), ('park', 'NN'), ('tree', 'NN'), ('rainbow', 'NN'), ('dogs', 'NNS'), ('puppies', 'NNS'), ('spotted', 'VBD'), ('saw', 'VBD'), ('dying', 'VBG'), ('love', 'VBP'), ('hate', 'VBP'), ('like', 'VBP'), ('is', 'VBZ'), ('last', 'JJ'), ('large', 'JJ'), ('new', 'JJ')]\n",
      "Count of unique words: 20\n"
     ]
    }
   ],
   "source": [
    "tags_unique = ['NN',\"NNP\",'NNS','NNPS','VB',\"VBD\",'VBG','VBN', 'VBP','VBZ','JJ',\"JJR\", 'JJS','RB','RBR','RBS']\n",
    "data_unique = find_words_pos(pos, tags_unique)\n",
    "print(\"Unique: \\n {}\".format(data_unique))\n",
    "print(\"Count of unique words: {}\".format(len(data_unique)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique: \n",
      " [('police', 'Noun'), ('puma', 'Noun'), ('night', 'Noun'), ('cat', 'Noun'), ('yesterday', 'Noun'), ('park', 'Noun'), ('tree', 'Noun'), ('rainbow', 'Noun'), ('dogs', 'Noun'), ('puppies', 'Noun'), ('spotted', 'Verb'), ('saw', 'Verb'), ('dying', 'Verb'), ('love', 'Verb'), ('hate', 'Verb'), ('like', 'Verb'), ('is', 'Verb'), ('last', 'JJRB'), ('large', 'JJRB'), ('new', 'JJRB')]\n",
      "Count of unique words: 20\n"
     ]
    }
   ],
   "source": [
    "tags_noun = ['NN',\"NNP\",'NNS','NNPS']\n",
    "tags_verb = ['VB',\"VBD\",'VBG','VBN', 'VBP','VBZ']\n",
    "tags_JJRB = ['JJ',\"JJR\", 'JJS','RB','RBR','RBS']\n",
    "for i,pos in enumerate(data_unique):\n",
    "    if pos[1] in tags_noun:\n",
    "        data_unique[i] = (data_unique[i][0],\"Noun\")\n",
    "    elif pos[1] in tags_verb:\n",
    "        data_unique[i] = (data_unique[i][0],\"Verb\")\n",
    "    elif pos[1] in tags_JJRB:\n",
    "        data_unique[i] = (data_unique[i][0],\"JJRB\")\n",
    "print(\"Unique: \\n {}\".format(data_unique))\n",
    "print(\"Count of unique words: {}\".format(len(data_unique)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the similarity between two words\n",
    "def similarity(w1,w2):\n",
    "    value = glove_model.similarity(w1,w2)\n",
    "    return [w1,w2,value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim(data_unique):\n",
    "    sim_list = []\n",
    "    for i, word1 in  enumerate(data_unique):\n",
    "        counter = 1\n",
    "        for j, word2 in enumerate(data_unique):\n",
    "            if i == j: continue\n",
    "            value = glove_model.similarity(word1[0],word2[0])\n",
    "            if value > 0.5: # Change the threshold\n",
    "                sim_list.append((word1[0],word1[1],word2[0],value))\n",
    "                counter += 1\n",
    "    return sim_list \n",
    "# sim_list[i][0] - word1\n",
    "# sim_list[i][1] - POS of word1\n",
    "# sim_list[i][2] - word2\n",
    "# sim_list[i][3] - similarity value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this method is used to count unique word that has similarity value > 0.5 within the corpus\n",
    "def count_corpus(sim_list):\n",
    "    unique = []\n",
    "    for word in sim_list:\n",
    "        if word[0] not in unique:\n",
    "            unique.append(word[0])\n",
    "    return len(unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('night', 'Noun', 'yesterday', 0.6068112818261747), ('night', 'Noun', 'saw', 0.5092502041657017), ('night', 'Noun', 'last', 0.6298856667608155), ('cat', 'Noun', 'dogs', 0.6921647999302281), ('cat', 'Noun', 'puppies', 0.5781784202694006), ('yesterday', 'Noun', 'night', 0.6068112818261747), ('yesterday', 'Noun', 'saw', 0.5903825419692302), ('yesterday', 'Noun', 'last', 0.7509270479250978), ('dogs', 'Noun', 'cat', 0.6921647999302281), ('dogs', 'Noun', 'puppies', 0.7978993201086826), ('puppies', 'Noun', 'cat', 0.5781784202694006), ('puppies', 'Noun', 'dogs', 0.7978993201086826), ('saw', 'Verb', 'night', 0.5092502041657017), ('saw', 'Verb', 'yesterday', 0.5903825419692302), ('saw', 'Verb', 'like', 0.5795228911657166), ('saw', 'Verb', 'last', 0.6219964413928809), ('love', 'Verb', 'hate', 0.6393098823113965), ('love', 'Verb', 'like', 0.65790401180881), ('hate', 'Verb', 'love', 0.6393098823113965), ('hate', 'Verb', 'like', 0.6574651482527226), ('like', 'Verb', 'saw', 0.5795228911657166), ('like', 'Verb', 'love', 0.65790401180881), ('like', 'Verb', 'hate', 0.6574651482527226), ('last', 'JJRB', 'night', 0.6298856667608155), ('last', 'JJRB', 'yesterday', 0.7509270479250978), ('last', 'JJRB', 'saw', 0.6219964413928809)]\n",
      "Unique words: 20\n",
      "Words that have similarity value > 0.5 within the corpus: 10\n"
     ]
    }
   ],
   "source": [
    "# Testing for finding similar words in unique word list\n",
    "print(sim(data_unique))\n",
    "print(\"Unique words: {}\".format(len(data_unique)))\n",
    "print(\"Words that have similarity value > 0.5 within the corpus: {}\".format(count_corpus(sim(data_unique))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_unique(final_set):\n",
    "    unique = []\n",
    "    unique_return = []\n",
    "    for i in final_set:\n",
    "        if i[0] not in unique:\n",
    "            unique.append(i[0])\n",
    "            unique_return.append((i[0],i[1]))\n",
    "    return unique_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of final set: 26\n",
      "Unique set: \n",
      " [('night', 'Noun'), ('cat', 'Noun'), ('yesterday', 'Noun'), ('dogs', 'Noun'), ('puppies', 'Noun'), ('saw', 'Verb'), ('love', 'Verb'), ('hate', 'Verb'), ('like', 'Verb'), ('last', 'JJRB')]\n",
      "Noun: \n",
      " [('night', 'Noun', 'yesterday', 0.6068112818261747), ('night', 'Noun', 'saw', 0.5092502041657017), ('night', 'Noun', 'last', 0.6298856667608155), ('cat', 'Noun', 'dogs', 0.6921647999302281), ('cat', 'Noun', 'puppies', 0.5781784202694006), ('yesterday', 'Noun', 'night', 0.6068112818261747), ('yesterday', 'Noun', 'saw', 0.5903825419692302), ('yesterday', 'Noun', 'last', 0.7509270479250978), ('dogs', 'Noun', 'cat', 0.6921647999302281), ('dogs', 'Noun', 'puppies', 0.7978993201086826), ('puppies', 'Noun', 'cat', 0.5781784202694006), ('puppies', 'Noun', 'dogs', 0.7978993201086826)]\n",
      "Verb: \n",
      " [('saw', 'Verb', 'night', 0.5092502041657017), ('saw', 'Verb', 'yesterday', 0.5903825419692302), ('saw', 'Verb', 'like', 0.5795228911657166), ('saw', 'Verb', 'last', 0.6219964413928809), ('love', 'Verb', 'hate', 0.6393098823113965), ('love', 'Verb', 'like', 0.65790401180881), ('hate', 'Verb', 'love', 0.6393098823113965), ('hate', 'Verb', 'like', 0.6574651482527226), ('like', 'Verb', 'saw', 0.5795228911657166), ('like', 'Verb', 'love', 0.65790401180881), ('like', 'Verb', 'hate', 0.6574651482527226)]\n",
      "Adjective & adverb: \n",
      " [('last', 'JJRB', 'night', 0.6298856667608155), ('last', 'JJRB', 'yesterday', 0.7509270479250978), ('last', 'JJRB', 'saw', 0.6219964413928809)]\n",
      "Count of Noun: 12\n",
      "Count of Verb: 11\n",
      "Count of Adjective & adverb: 3\n"
     ]
    }
   ],
   "source": [
    "# this part is for the CGT project\n",
    "final_set = sim(data_unique)\n",
    "final_unique = find_unique(final_set)\n",
    "data_noun = find_words_pos(final_set, ['Noun'])\n",
    "data_verb = find_words_pos(final_set, ['Verb'])\n",
    "data_JJRB = find_words_pos(final_set, ['JJRB'])\n",
    "# word = find_words(data, 'word')\n",
    "# tag = find_words(data, 'pos')\n",
    "print(\"Count of final set: {}\".format(len(final_set)))\n",
    "print(\"Unique set: \\n {}\".format(final_unique))\n",
    "print(\"Noun: \\n {}\".format(data_noun))\n",
    "print(\"Verb: \\n {}\".format(data_verb))\n",
    "print(\"Adjective & adverb: \\n {}\".format(data_JJRB))\n",
    "print(\"Count of Noun: {}\".format(len(data_noun)))\n",
    "print(\"Count of Verb: {}\".format(len(data_verb)))\n",
    "print(\"Count of Adjective & adverb: {}\".format(len(data_JJRB)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "#to Json file\n",
    "word = {\n",
    "        'name': \"Words\",\n",
    "        'children': []\n",
    "        }\n",
    "noun = {\n",
    "        'name': \"NOUN\",\n",
    "        'children': []\n",
    "        }\n",
    "verb = {\n",
    "        'name': \"VERB\",\n",
    "        'children': []\n",
    "        }\n",
    "JJRB = {\n",
    "        'name': \"ADV_ADJ\",\n",
    "        'children': []\n",
    "        }\n",
    "similarWords = {\n",
    "            'name': \"\",\n",
    "            'children': []\n",
    "            }\n",
    "with open('data.json','w') as out:\n",
    "    for unique in final_unique: \n",
    "        if unique[1] == \"Noun\":\n",
    "            uniqueWord = {\n",
    "                        'name': \"\",\n",
    "                        'children': []\n",
    "                        }\n",
    "            for similar in data_noun:\n",
    "                if unique[0] == similar[0]:\n",
    "                    similarWords = {\n",
    "                        'name': similar[2],\n",
    "                        'similarity': similar[3]\n",
    "                    }\n",
    "                    uniqueWord['name'] = unique[0]\n",
    "                    #print(unique[0])\n",
    "                    #print(similarWords)\n",
    "                    uniqueWord['children'].append(similarWords)\n",
    "                    #print(uniqueWord)\n",
    "            noun['children'].append(uniqueWord)\n",
    "            # print(noun)\n",
    "        elif unique[1] == \"Verb\":\n",
    "            uniqueWord = {\n",
    "                        'name': \"\",\n",
    "                        'children': []\n",
    "                        }\n",
    "            for similar in data_verb:\n",
    "                if unique[0] == similar[0]:\n",
    "                    similarWords = {\n",
    "                        'name': similar[2],\n",
    "                        'similarity': similar[3]\n",
    "                    }\n",
    "                    uniqueWord['name'] = unique[0]\n",
    "                    #print(unique[0])\n",
    "                    #print(similarWords)\n",
    "                    uniqueWord['children'].append(similarWords)\n",
    "                    #print(uniqueWord)\n",
    "            verb['children'].append(uniqueWord)\n",
    "            # print(noun)\n",
    "        elif unique[1] == \"JJRB\":\n",
    "            uniqueWord = {\n",
    "                        'name': \"\",\n",
    "                        'children': []\n",
    "                        }\n",
    "            for similar in data_JJRB:\n",
    "                if unique[0] == similar[0]:\n",
    "                    similarWords = {\n",
    "                        'name': similar[2],\n",
    "                        'similarity': similar[3]\n",
    "                    }\n",
    "                    uniqueWord['name'] = unique[0]\n",
    "                    #print(unique[0])\n",
    "                    #print(similarWords)\n",
    "                    uniqueWord['children'].append(similarWords)\n",
    "                    #print(uniqueWord)\n",
    "            JJRB['children'].append(uniqueWord)\n",
    "            # print(noun)\n",
    "            \n",
    "\n",
    "    word['children'].append(noun)\n",
    "    word['children'].append(verb)\n",
    "    word['children'].append(JJRB)\n",
    "    json.dump(word, out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
